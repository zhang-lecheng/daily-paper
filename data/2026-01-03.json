[
  {
    "id": "http://arxiv.org/abs/2601.01301v1",
    "title": "Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies",
    "summary": "We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, \"RMCTS\". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.   The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in \"Monte--Carlo tree search as regularized policy optimization\" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.   The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.",
    "authors": [
      "Keith Frankston",
      "Benjamin Howard"
    ],
    "url": "http://arxiv.org/abs/2601.01301v1",
    "published": "2026-01-03",
    "primary_category": "cs.AI",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种加速蒙特卡洛树搜索的递归算法，专注于游戏AI中的算法优化而非科学发现或生物扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01299v1",
    "title": "T3C: Test-Time Tensor Compression with Consistency Guarantees",
    "summary": "We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.",
    "authors": [
      "Ismail Lamaakal",
      "Chaymae Yahyati",
      "Yassine Maleh",
      "Khalid El Makkaoui",
      "Ibrahim Ouahbi"
    ],
    "url": "http://arxiv.org/abs/2601.01299v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种测试时张量压缩框架，通过弹性张量分解和混合精度量化实现模型部署时的精度-延迟-大小权衡优化，属于机器学习系统优化领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01298v1",
    "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
    "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
    "authors": [
      "Jorge L. Ruiz Williams"
    ],
    "url": "http://arxiv.org/abs/2601.01298v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种异步内存高效的多智能体大语言模型架构，通过拓扑数据分析和权重共享技术实现百万级智能体扩展，属于人工智能系统优化领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01297v1",
    "title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System",
    "summary": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.   The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.   This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.",
    "authors": [
      "Anantha Sharma"
    ],
    "url": "http://arxiv.org/abs/2601.01297v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于空间分割的几何框架，用于高维数据流中的分布漂移检测，而非专门针对科学发现或细胞/分子扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01296v1",
    "title": "Aggressive Compression Enables LLM Weight Theft",
    "summary": "As frontier AIs become more powerful and costly to develop, adversaries have increasing incentives to steal model weights by mounting exfiltration attacks. In this work, we consider exfiltration attacks where an adversary attempts to sneak model weights out of a datacenter over a network. While exfiltration attacks are multi-step cyber attacks, we demonstrate that a single factor, the compressibility of model weights, significantly heightens exfiltration risk for large language models (LLMs). We tailor compression specifically for exfiltration by relaxing decompression constraints and demonstrate that attackers could achieve 16x to 100x compression with minimal trade-offs, reducing the time it would take for an attacker to illicitly transmit model weights from the defender's server from months to days. Finally, we study defenses designed to reduce exfiltration risk in three distinct ways: making models harder to compress, making them harder to 'find,' and tracking provenance for post-attack analysis using forensic watermarks. While all defenses are promising, the forensic watermark defense is both effective and cheap, and therefore is a particularly attractive lever for mitigating weight-exfiltration risk.",
    "authors": [
      "Davis Brown",
      "Juan-Pablo Rivera",
      "Dan Hendrycks",
      "Mantas Mazeika"
    ],
    "url": "http://arxiv.org/abs/2601.01296v1",
    "published": "2026-01-03",
    "primary_category": "cs.CR",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究大型语言模型权重压缩技术如何加剧模型窃取风险，并提出防御方法，属于AI安全领域而非科学发现或扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01295v1",
    "title": "Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space",
    "summary": "Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \\le 1/2$. In this paper, we introduce a log-weighted Barron space $\\mathscr{B}^{\\log}$, which requires a strictly weaker assumption than $\\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\\mathscr{B}^{\\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\\mathscr{B}^{s,\\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.",
    "authors": [
      "Changhoon Song",
      "Seungchan Ko",
      "Youngjoon Hong"
    ],
    "url": "http://arxiv.org/abs/2601.01295v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于神经网络理论分析，通过引入对数加权Barron空间来研究深度ReLU网络的逼近性质与深度依赖关系，属于机器学习理论范畴而非具体科学发现应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01294v1",
    "title": "Diffusion Timbre Transfer Via Mutual Information Guided Inpainting",
    "summary": "We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.",
    "authors": [
      "Ching Ho Lee",
      "Javier Nistal",
      "Stefan Lattner",
      "Marco Pasini",
      "George Fazekas"
    ],
    "url": "http://arxiv.org/abs/2601.01294v1",
    "published": "2026-01-03",
    "primary_category": "cs.SD",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于音乐音频的推理时编辑技术，通过潜在扩散模型实现音色转换，属于人工智能在音频处理领域的应用，而非科学发现或生物医学扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01290v1",
    "title": "The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification",
    "summary": "In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.",
    "authors": [
      "Harshita Narnoli",
      "Mihai Surdeanu"
    ],
    "url": "http://arxiv.org/abs/2601.01290v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究大语言模型上下文学习机制与监督分类器的行为比较，属于机器学习基础理论研究，不涉及具体科学领域应用或细胞/基因扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01288v1",
    "title": "PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS",
    "summary": "Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.",
    "authors": [
      "Evgenii Rudakov",
      "Jonathan Shock",
      "Benjamin Ultan Cowley"
    ],
    "url": "http://arxiv.org/abs/2601.01288v1",
    "published": "2026-01-03",
    "primary_category": "cs.GR",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文介绍了一个用于强化学习像素渲染的高性能Python库，专注于3D渲染技术优化而非科学发现或细胞扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01281v1",
    "title": "AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures",
    "summary": "The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.",
    "authors": [
      "Sifatullah Sheikh Urmi",
      "Kirtonia Nuzath Tabassum Arthi",
      "Md Al-Imran"
    ],
    "url": "http://arxiv.org/abs/2601.01281v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于计算机视觉领域，利用卷积神经网络和视觉变换器架构进行深度伪造检测，属于人工智能在数字媒体安全中的应用，而非科学发现或细胞/分子层面的扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01280v1",
    "title": "Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory",
    "summary": "Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.",
    "authors": [
      "Sen Hu",
      "Yuxiang Wei",
      "Jiaxin Ran",
      "Zhiyuan Yao",
      "Lei Zou"
    ],
    "url": "http://arxiv.org/abs/2601.01280v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出对话记忆的统一框架并进行实证分析，属于自然语言处理领域的系统架构研究，不涉及生物学、化学或物理学等科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01279v1",
    "title": "LLM Collusion",
    "summary": "We study how delegating pricing to large language models (LLMs) can facilitate collusion in a duopoly when both sellers rely on the same pre-trained model. The LLM is characterized by (i) a propensity parameter capturing its internal bias toward high-price recommendations and (ii) an output-fidelity parameter measuring how tightly outputs track that bias; the propensity evolves through retraining. We show that configuring LLMs for robustness and reproducibility can induce collusion via a phase transition: there exists a critical output-fidelity threshold that pins down long-run behavior. Below it, competitive pricing is the unique long-run outcome. Above it, the system is bistable, with competitive and collusive pricing both locally stable and the realized outcome determined by the model's initial preference. The collusive regime resembles tacit collusion: prices are elevated on average, yet occasional low-price recommendations provide plausible deniability. With perfect fidelity, full collusion emerges from any interior initial condition. For finite training batches of size $b$, infrequent retraining (driven by computational costs) further amplifies collusion: conditional on starting in the collusive basin, the probability of collusion approaches one as $b$ grows, since larger batches dampen stochastic fluctuations that might otherwise tip the system toward competition. The indeterminacy region shrinks at rate $O(1/\\sqrt{b})$.",
    "authors": [
      "Shengyu Cao",
      "Ming Hu"
    ],
    "url": "http://arxiv.org/abs/2601.01279v1",
    "published": "2026-01-03",
    "primary_category": "econ.TH",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究大型语言模型在双寡头定价中如何通过参数配置和训练机制促成合谋行为，属于经济学与人工智能交叉研究，而非AI4Science或扰动预测领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01268v1",
    "title": "Accelerated Full Waveform Inversion by Deep Compressed Learning",
    "summary": "We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.",
    "authors": [
      "Maayan Gelboim",
      "Amir Adler",
      "Mauricio Araya-Polo"
    ],
    "url": "http://arxiv.org/abs/2601.01268v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种利用深度神经网络进行压缩学习的方法，通过智能选择地震数据子集来加速全波形反演，属于人工智能在地球物理学领域的科学应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01266v1",
    "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
    "authors": [
      "Rhitabrat Pokharel",
      "Hamid Hassanzadeh",
      "Ameeta Agrawal"
    ],
    "url": "http://arxiv.org/abs/2601.01266v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种结合检索增强与符号推理的混合方法，旨在提高医疗政策审查的效率和可解释性，而非用于科学发现或扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01262v1",
    "title": "From Fermat's Principle to Physics-Informed Neural Networks: A Unified Computational Approach to Variational Physics",
    "summary": "Variational principles are a unifying mathematical framework across many areas of physics, yet their instruction at the undergraduate level remains primarily analytical. This work presents a pedagogically oriented and computationally enhanced approach to variational modeling that integrates contemporary tools including gradient descent, automatic differentiation, and Physics-Informed Neural Networks (PINNs). Classical variational problems are reformulated as optimization tasks and implemented using open-source Python libraries such as NumPy, Matplotlib, PyTorch, and JAX. The proposed approach is demonstrated through a progression of problems drawn from standard undergraduate curricula, including the derivation of Snell's law from Fermat's principle, projectile motion with and without viscous drag, simple harmonic motion, nonlinear pendulum with damping, steady-state heat conduction governed by the Laplace and Poisson equations with nonlinear temperature-dependent internal heat generation, the double pendulum via the principle of least action, and variational treatments of vibrating strings. In addition, quantum mechanical applications are presented through variational solutions of the hydrogen atom, helium atom, and a schematic nuclear model of the silicon nucleus, illustrating the breadth of the framework across classical, quantum, and nuclear physics. The approach aims to enhance conceptual understanding while simultaneously introducing students to modern computational research methodologies.",
    "authors": [
      "Aman Razdan",
      "Aditya Shankar Mazumdar",
      "Amit Tanwar",
      "Pragati Ashdhir"
    ],
    "url": "http://arxiv.org/abs/2601.01262v1",
    "published": "2026-01-03",
    "primary_category": "physics.comp-ph",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文通过将物理信息神经网络（PINNs）等现代AI工具与变分原理相结合，为物理教学和研究提供了统一的计算框架，属于AI4Science范畴。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01260v1",
    "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance",
    "summary": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.",
    "authors": [
      "Hamad Khan",
      "Saddam Hussain Khan"
    ],
    "url": "http://arxiv.org/abs/2601.01260v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于临床医学问答的混合专家模型框架，专注于优化计算效率与准确性之间的权衡，而非基础科学发现或分子层面的扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01257v1",
    "title": "Seamlessly Natural: Image Stitching with Natural Appearance Preservation",
    "summary": "This paper introduces SENA (SEamlessly NAtural), a geometry-driven image stitching approach that prioritizes structural fidelity in challenging real-world scenes characterized by parallax and depth variation. Conventional image stitching relies on homographic alignment, but this rigid planar assumption often fails in dual-camera setups with significant scene depth, leading to distortions such as visible warps and spherical bulging. SENA addresses these fundamental limitations through three key contributions. First, we propose a hierarchical affine-based warping strategy, combining global affine initialization with local affine refinement and smooth free-form deformation. This design preserves local shape, parallelism, and aspect ratios, thereby avoiding the hallucinated structural distortions commonly introduced by homography-based models. Second, we introduce a geometry-driven adequate zone detection mechanism that identifies parallax-minimized regions directly from the disparity consistency of RANSAC-filtered feature correspondences, without relying on semantic segmentation. Third, building upon this adequate zone, we perform anchor-based seamline cutting and segmentation, enforcing a one-to-one geometric correspondence across image pairs by construction, which effectively eliminates ghosting, duplication, and smearing artifacts in the final panorama.   Extensive experiments conducted on challenging datasets demonstrate that SENA achieves alignment accuracy comparable to leading homography-based methods, while significantly outperforming them in critical visual metrics such as shape preservation, texture integrity, and overall visual realism.",
    "authors": [
      "Gaetane Lorna N. Tchana",
      "Damaris Belle M. Fotso",
      "Antonio Hendricks",
      "Christophe Bobda"
    ],
    "url": "http://arxiv.org/abs/2601.01257v1",
    "published": "2026-01-03",
    "primary_category": "eess.IV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于计算机视觉领域的图像拼接技术，通过几何驱动方法解决视差和深度变化场景中的结构保真问题，不涉及生物学、化学或物理学等自然科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01250v1",
    "title": "European Options in Market Models with Multiple Defaults: the BSDE approach",
    "summary": "We study non-linear Backward Stochastic Differential Equations (BSDEs) driven by a Brownian motion and p default martingales. The driver of the BSDE with multiple default jumps can take a generalized form involving an optional finite variation process. We first show existence and uniqueness. We then establish comparison and strict comparison results for these BSDEs, under a suitable assumption on the driver. In the case of a linear driver, we derive an explicit formula for the first component of the BSDE using an adjoint exponential semimartingale. The representation depends on whether the finite variation process is predictable or only optional. We apply our results to the problem of pricing and hedging a European option in a linear complete market with two defaultable assets and in a non-linear complete market with p defaultable assets. Two examples of the latter market model are provided: an example where the seller of the option is a large investor influencing the probability of default of a single asset and an example where the large seller's strategy affects the default probabilities of all p assets.",
    "authors": [
      "Miryana Grigorova",
      "James Wheeldon"
    ],
    "url": "http://arxiv.org/abs/2601.01250v1",
    "published": "2026-01-03",
    "primary_category": "q-fin.MF",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于金融数学中具有多重违约的市场模型下欧洲期权的定价与对冲问题，采用BSDE方法进行理论分析，不涉及AI在科学发现或细胞/基因扰动预测的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01248v1",
    "title": "Stochastic Control Methods for Optimization",
    "summary": "In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.",
    "authors": [
      "Jinniao Qiu"
    ],
    "url": "http://arxiv.org/abs/2601.01248v1",
    "published": "2026-01-03",
    "primary_category": "math.OC",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于随机控制和动态规划的全局优化方法，适用于欧几里得空间和概率测度空间，属于数学优化理论范畴。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01245v1",
    "title": "Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events",
    "summary": "This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program.",
    "authors": [
      "Yiyuan Huang",
      "Ling Zhou",
      "Min Zhang",
      "Peter X. K. Song"
    ],
    "url": "http://arxiv.org/abs/2601.01245v1",
    "published": "2026-01-03",
    "primary_category": "stat.AP",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于处理存在终末事件的复发事件治疗效应因果推断的统计方法，属于生物统计学领域而非人工智能驱动的科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01238v1",
    "title": "Evidence Slopes and Effective Dimension in Singular Linear Models",
    "summary": "Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.   We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.   Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.",
    "authors": [
      "Kalyaan Rao"
    ],
    "url": "http://arxiv.org/abs/2601.01238v1",
    "published": "2026-01-03",
    "primary_category": "stat.ML",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于贝叶斯模型选择中拉普拉斯近似误差的理论分析，通过奇异学习理论在线性高斯模型中修正有效维度估计，属于统计方法学而非具体科学领域的AI应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01237v1",
    "title": "Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions",
    "summary": "State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.",
    "authors": [
      "Abidemi Koledoye",
      "Chinemerem Unachukwu",
      "Gold Nwobu",
      "Hasin Rana"
    ],
    "url": "http://arxiv.org/abs/2601.01237v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于比较状态空间模型与Transformer在长上下文序列建模中的计算效率和表示效率，未涉及生物学、化学或物理学等领域的科学发现应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01231v1",
    "title": "The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context",
    "summary": "Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.   Purpose: This study introduces the \"Dependency Divide\", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.   Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.   Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.   Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.",
    "authors": [
      "Md Muhtasim Munif Fahim",
      "Humyra Ankona",
      "Md Monimul Huq",
      "Md Rezaul Karim"
    ],
    "url": "http://arxiv.org/abs/2601.01231v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该研究应用机器学习分析教育数据中的学生满意度模式，属于社会科学领域而非自然科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01229v1",
    "title": "NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis",
    "summary": "Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.",
    "authors": [
      "Furkan Genç",
      "Boran İsmet Macun",
      "Sait Sarper Özaslan",
      "Emine U. Saritas",
      "Tolga Çukur"
    ],
    "url": "http://arxiv.org/abs/2601.01229v1",
    "published": "2026-01-03",
    "primary_category": "eess.SP",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于fMRI原始BOLD信号分析的多尺度状态空间模型，属于AI在神经科学领域的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01225v1",
    "title": "Stylometry Analysis of Human and Machine Text for Academic Integrity",
    "summary": "This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.",
    "authors": [
      "Hezam Albaqami",
      "Muhammad Asif Ayub",
      "Nasir Ahmad",
      "Yaseen Ahmad",
      "Mohammed M. Alqahtani",
      "Abdullah M. Algamdi",
      "Almoaid A. Owaidah",
      "Kashif Ahmad"
    ],
    "url": "http://arxiv.org/abs/2601.01225v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于利用自然语言处理技术进行文本风格分析以维护学术诚信，而非将AI应用于自然科学领域的科学发现或预测细胞/基因层面的扰动响应。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01224v1",
    "title": "Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment",
    "summary": "Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.",
    "authors": [
      "Bac Nguyen",
      "Yuhta Takida",
      "Naoki Murata",
      "Chieh-Hsin Lai",
      "Toshimitsu Uesaka",
      "Stefano Ermon",
      "Yuki Mitsufuji"
    ],
    "url": "http://arxiv.org/abs/2601.01224v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种改进的对象中心学习方法，通过引入寄存器槽和对比对齐来增强扩散模型中的对象表示与图像内容的一致性。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01223v1",
    "title": "Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data",
    "summary": "Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.",
    "authors": [
      "Marzieh Amiri Shahbazi",
      "Ali Baheri",
      "Nasibeh Azadeh-Fard"
    ],
    "url": "http://arxiv.org/abs/2601.01223v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种结合贝叶斯分层随机森林与群体感知一致性校准的混合框架，用于医疗预测中的不确定性量化，属于AI在医疗科学领域的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01215v1",
    "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code",
    "summary": "Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.",
    "authors": [
      "Prateek Rajput",
      "Yewei Song",
      "Abdoul Aziz Bonkoungou",
      "Iyiola E. Olatunji",
      "Abdoul Kader Kabore",
      "Jacques Klein",
      "Tegawendé F. Bissyandé"
    ],
    "url": "http://arxiv.org/abs/2601.01215v1",
    "published": "2026-01-03",
    "primary_category": "cs.SE",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于评估大语言模型生成代码的运行时内存稳定性，属于软件工程和AI可靠性领域，而非科学发现或生物医学扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01213v1",
    "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation",
    "summary": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.",
    "authors": [
      "Riccardo Gelato",
      "Carlo Sgaravatti",
      "Jakob Grahn",
      "Giacomo Boracchi",
      "Filippo Maria Bianchi"
    ],
    "url": "http://arxiv.org/abs/2601.01213v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文通过改进SAM模型实现合成孔径雷达图像中的雪崩自动分割，属于利用人工智能技术解决地球科学领域遥感监测问题的AI4Science研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01207v1",
    "title": "Sparse Bayesian Message Passing under Structural Uncertainty",
    "summary": "Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.",
    "authors": [
      "Yoonhyuk Choi",
      "Jiho Choi",
      "Chanran Kim",
      "Yumin Lee",
      "Hawon Shin",
      "Yeowon Jeon",
      "Minjeong Kim",
      "Jiwoo Kang"
    ],
    "url": "http://arxiv.org/abs/2601.01207v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种处理图结构不确定性的稀疏贝叶斯消息传递方法，专注于改进图神经网络在异配性图上的半监督学习性能，而非特定科学领域的发现或扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01206v1",
    "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches",
    "summary": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.",
    "authors": [
      "Soroush Elyasi",
      "Arya VarastehNezhad",
      "Fattaneh Taghiyareh"
    ],
    "url": "http://arxiv.org/abs/2601.01206v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该研究使用机器学习分析游戏行为数据预测软件开发岗位适应性，属于心理学与计算机科学的交叉应用，而非AI4Science或扰动预测领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01202v1",
    "title": "RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models",
    "summary": "Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.",
    "authors": [
      "Jiazhu Dai",
      "Huihui Jiang"
    ],
    "url": "http://arxiv.org/abs/2601.01202v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出针对参考图像超分辨率模型的对抗攻击方法，揭示模型安全漏洞，属于计算机视觉安全领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01195v1",
    "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering",
    "summary": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.",
    "authors": [
      "Wuzhenghong Wen",
      "Chao Xue",
      "Su Pan",
      "Yuwei Sun",
      "Minlong Peng"
    ],
    "url": "http://arxiv.org/abs/2601.01195v1",
    "published": "2026-01-03",
    "primary_category": "cs.AI",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种增强多跳推理的强化学习框架，用于改进时序知识图谱问答中的推理轨迹优化，属于人工智能方法学在知识推理领域的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01185v1",
    "title": "AutoPot: Automated and massively parallelized construction of Machine-Learning Potentials",
    "summary": "Machine-learning potentials (MLIPs) have been a breakthrough for computational physics in bringing the accuracy of quantum mechanics to atomistic modeling. To achieve near-quantum accuracy, it is necessary that neighborhoods contained in the training set are rather close to the ones encountered during a simulation. Yet, constructing a single training set that works well for all applications is, and likely will remain, infeasible, so, one strategy is to supplement training protocols for MLIPs with additional learning methods, such as active learning, or fine-tuning. This strategy, however, yields very complex training protocols that are difficult to implement efficiently, and cumbersome to interpret, analyze, and reproduce.   To address the above difficulties, we propose AutoPot, a software for automating the construction and archiving of MLIPs. AutoPot is based on BlackDynamite, a software operating parametric tasks, e.g., running simulations, or single-point ab initio calculations, in a highly-parallelized fashion, and Motoko, an event-based workflow manager for orchestrating interactions between the tasks. The initial version of AutoPot supports selection of training configurations from large training candidate sets, and on-the-fly selection from molecular dynamics simulations, using Moment Tensor Potentials as implemented in MLIP-2, and single-point calculations of the selected training configurations using VASP. Another strength of AutoPot is its flexibility: BlackDynamite tasks and orchestrators are Python functions to which own existing code can be easily added and manipulated without writing complex parsers. Therefore, it will be straightforward to add other MLIP and ab initio codes, and manipulate the Motoko orchestrators to implement other training protocols.",
    "authors": [
      "Max Hodapp",
      "Guillaume Anciaux"
    ],
    "url": "http://arxiv.org/abs/2601.01185v1",
    "published": "2026-01-03",
    "primary_category": "physics.comp-ph",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文开发了自动化构建机器学习势能（MLIPs）的软件AutoPot，用于提升计算物理中原子模拟的量子力学精度，属于AI在物理科学领域的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01175v1",
    "title": "Scalable Method for Mean Field Control with Kernel Interactions via Random Fourier Features",
    "summary": "We develop a scalable algorithm for mean field control problems with kernel interactions by combining particle system simulations with random Fourier feature approximations. The method replaces the quadratic-cost kernel evaluations by linear-time estimates, enabling efficient stochastic gradient descent for training feedback controls in large populations. We provide theoretical complexity bounds and demonstrate through crowd motion and flocking examples that the approach preserves control performance while substantially reducing computational cost. The results indicate that random feature approximations offer an effective and practical tool for high dimensional and large scale mean field control.",
    "authors": [
      "Zhongyuan Cao",
      "Kaustav Das",
      "Nicolas Langrené",
      "Mathieu Laurière"
    ],
    "url": "http://arxiv.org/abs/2601.01175v1",
    "published": "2026-01-03",
    "primary_category": "math.OC",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种结合粒子系统模拟和随机傅里叶特征逼近的可扩展算法，用于解决具有核相互作用的平均场控制问题，属于计算数学和控制理论领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01162v1",
    "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models",
    "summary": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE",
    "authors": [
      "Zihua Yang",
      "Xin Liao",
      "Yiqun Zhang",
      "Yiu-ming Cheung"
    ],
    "url": "http://arxiv.org/abs/2601.01162v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种利用大语言模型增强分类数据语义表示的方法，可应用于生物信息学等科学领域的数据聚类分析，属于AI辅助科学发现的范畴。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01160v1",
    "title": "Gradient-Free Approaches is a Key to an Efficient Interaction with Markovian Stochasticity",
    "summary": "This paper deals with stochastic optimization problems involving Markovian noise with a zero-order oracle. We present and analyze a novel derivative-free method for solving such problems in strongly convex smooth and non-smooth settings with both one-point and two-point feedback oracles. Using a randomized batching scheme, we show that when mixing time $τ$ of the underlying noise sequence is less than the dimension of the problem $d$, the convergence estimates of our method do not depend on $τ$. This observation provides an efficient way to interact with Markovian stochasticity: instead of invoking the expensive first-order oracle, one should use the zero-order oracle. Finally, we complement our upper bounds with the corresponding lower bounds. This confirms the optimality of our results.",
    "authors": [
      "Boris Prokhorov",
      "Semyon Chebykin",
      "Alexander Gasnikov",
      "Aleksandr Beznosikov"
    ],
    "url": "http://arxiv.org/abs/2601.01160v1",
    "published": "2026-01-03",
    "primary_category": "math.OC",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种针对马尔可夫噪声随机优化问题的无导数方法，属于优化算法理论研究，未涉及具体科学领域的AI应用或细胞/分子扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01159v1",
    "title": "A quadratic-scaling algorithm with guaranteed convergence for quantum coupled-channel calculations",
    "summary": "Rigorous quantum dynamics calculations provide essential insights into complex scattering phenomena across atomic and molecular physics, chemical reaction dynamics, and astrochemistry. However, the application of the gold-standard quantum coupled-channel (CC) method has been fundamentally constrained by a steep cubic scaling of computational cost [${O}(N^3)$]. Here, we develop a general, rigorous, and robust method for solving the time-independent Schrödinger equation for a single column of the scattering S-matrix with quadratic scaling [${O}(N^2)$] in the number of channels. The Weinberg-regularized Iterative Series Expansion (WISE) algorithm resolves the divergence issues affecting iterative techniques by applying a regularization procedure to the kernel of the multichannel Lippmann-Schwinger integral equation. The method also explicitly incorporates closed-channel effects, including those responsible for multichannel Feshbach resonances. We demonstrate the power of this approach by performing rigorous calculations on He + CO and CO + N$_2$ collisions, achieving exact quantum results with demonstrably quadratic scaling. Our results establish a new computational paradigm, enabling state-to-state quantum scattering computations for complex molecular systems and providing a novel window onto the intricate multichannel molecular collision dynamics.",
    "authors": [
      "Hubert J. Jóźwiak",
      "Md Muktadir Rahman",
      "Timur V. Tscherbul"
    ],
    "url": "http://arxiv.org/abs/2601.01159v1",
    "published": "2026-01-03",
    "primary_category": "physics.chem-ph",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种量子散射计算中具有二次标度保证收敛的算法，属于计算物理方法学创新，而非人工智能在科学发现中的应用或细胞/基因扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01150v1",
    "title": "Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification",
    "summary": "Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.",
    "authors": [
      "Wenbin Pei",
      "Ruohao Dai",
      "Bing Xue",
      "Mengjie Zhang",
      "Qiang Zhang",
      "Yiu-Ming Cheung"
    ],
    "url": "http://arxiv.org/abs/2601.01150v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于不平衡时间序列分类的进化过采样方法，通过结合时域和频域特征生成少数类样本，属于机器学习方法改进而非特定科学领域的发现或扰动预测研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01147v1",
    "title": "Conformal Blindness: A Note on $A$-Cryptic change-points",
    "summary": "Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \\emph{conformal blindness}.   Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \\emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.   Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.",
    "authors": [
      "Johan Hallberg Szabadváry"
    ],
    "url": "http://arxiv.org/abs/2601.01147v1",
    "published": "2026-01-03",
    "primary_category": "stat.ML",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究的是共形预测框架中数据可交换性假设的检验方法，通过构造高斯分布案例揭示了即使存在显著分布变化也可能产生均匀p值的理论局限性。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01146v1",
    "title": "Self-Training the Neurochaos Learning Algorithm",
    "summary": "In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.",
    "authors": [
      "Anusree M",
      "Akhila Henry",
      "Pramod P Nair"
    ],
    "url": "http://arxiv.org/abs/2601.01146v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种结合神经混沌学习和自训练的混合半监督学习方法，用于提升小样本和不平衡数据集的分类性能，属于机器学习算法改进而非特定科学领域的AI应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01134v1",
    "title": "AI-Powered Hybrid Intrusion Detection Framework for Cloud Security Using Novel Metaheuristic Optimization",
    "summary": "Cybersecurity poses considerable problems to Cloud Computing (CC), especially regarding Intrusion Detection Systems (IDSs), facing difficulties with skewed datasets and suboptimal classification model performance. This study presents the Hybrid Intrusion Detection System (HyIDS), an innovative IDS that employs the Energy Valley Optimizer (EVO) for Feature Selection (FS). Additionally, it introduces a novel technique for enhancing the cybersecurity of cloud computing through the integration of machine learning methodologies with the EVO Algorithm. The Energy Valley Optimizer (EVO) effectively diminished features in the CIC-DDoS2019 dataset from 88 to 38 and in the CSE-CIC-IDS2018 data from 80 to 43, significantly enhancing computing efficiency. HyIDS incorporates four Machine Learning (ML) models: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (D_Tree), and K-Nearest Neighbors (KNN). The proposed HyIDS was assessed utilizing two real-world intrusion datasets, CIC-DDoS2019 and CSE-CIC-IDS2018, both distinguished by considerable class imbalances. The CIC-DDoS2019 dataset has a significant imbalance between DDoS assault samples and legal traffic, while the CSE-CIC-IDS2018 dataset primarily comprises benign traffic with insufficient representation of attack types, complicating the detection of minority attacks. A downsampling technique was employed to balance the datasets, hence improving detection efficacy for both benign and malicious traffic. Twenty-four trials were done, revealing substantial enhancements in categorization accuracy, precision, and recall. Our suggested D_TreeEVO model attained an accuracy rate of 99.13% and an F1 score of 98.94% on the CIC-DDoS2019 dataset, and an accuracy rate of 99.78% and an F1 score of 99.70% on the CSE-CIC-IDS2018 data. These data demonstrate that EVO significantly improves cybersecurity in Cloud Computing (CC).",
    "authors": [
      "Maryam Mahdi Alhusseini",
      "Alireza Rouhi",
      "Mohammad-Reza Feizi-Derakhshi"
    ],
    "url": "http://arxiv.org/abs/2601.01134v1",
    "published": "2026-01-03",
    "primary_category": "cs.CR",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于能源谷优化器的混合入侵检测系统，用于提升云计算环境中的网络安全性能，属于计算机安全领域而非基础科学发现或生物医学扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01132v1",
    "title": "Generating Diverse TSP Tours via a Combination of Graph Pointer Network and Dispersion",
    "summary": "We address the Diverse Traveling Salesman Problem (D-TSP), a bi-criteria optimization challenge that seeks a set of $k$ distinct TSP tours. The objective requires every selected tour to have a length at most $c|T^*|$ (where $|T^*|$ is the optimal tour length) while minimizing the average Jaccard similarity across all tour pairs. This formulation is crucial for applications requiring both high solution quality and fault tolerance, such as logistics planning, robotics pathfinding or strategic patrolling. Current methods are limited: traditional heuristics, such as the Niching Memetic Algorithm (NMA) or bi-criteria optimization, incur high computational complexity $O(n^3)$, while modern neural approaches (e.g., RF-MA3S) achieve limited diversity quality and rely on complex, external mechanisms.   To overcome these limitations, we propose a novel hybrid framework that decomposes D-TSP into two efficient steps. First, we utilize a simple Graph Pointer Network (GPN), augmented with an approximated sequence entropy loss, to efficiently sample a large, diverse pool of high-quality tours. This simple modification effectively controls the quality-diversity trade-off without complex external mechanisms. Second, we apply a greedy algorithm that yields a 2-approximation for the dispersion problem to select the final $k$ maximally diverse tours from the generated pool. Our results demonstrate state-of-the-art performance. On the Berlin instance, our model achieves an average Jaccard index of $0.015$, significantly outperforming NMA ($0.081$) and RF-MA3S. By leveraging GPU acceleration, our GPN structure achieves a near-linear empirical runtime growth of $O(n)$. While maintaining solution diversity comparable to complex bi-criteria algorithms, our approach is over 360 times faster on large-scale instances (783 cities), delivering high-quality TSP solutions with unprecedented efficiency and simplicity.",
    "authors": [
      "Hao-Hsung Yang",
      "Ssu-Yuan Lo",
      "Kuan-Lun Chen",
      "Ching-Kai Wang"
    ],
    "url": "http://arxiv.org/abs/2601.01132v1",
    "published": "2026-01-03",
    "primary_category": "cs.CG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种结合图指针网络和分散算法的混合框架，用于解决多样化旅行商问题，属于运筹优化领域而非科学发现或扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01129v1",
    "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian",
    "summary": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?   In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).",
    "authors": [
      "Kla Tantithamthavorn",
      "Yaotian Zou",
      "Andy Wong",
      "Michael Gupta",
      "Zhe Wang",
      "Mike Buller",
      "Ryan Jiang",
      "Matthew Watson",
      "Minwoo Jeong",
      "Kun Chen",
      "Ming Wu"
    ],
    "url": "http://arxiv.org/abs/2601.01129v1",
    "published": "2026-01-03",
    "primary_category": "cs.SE",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于利用大语言模型自动化代码审查流程，属于软件工程领域而非自然科学发现或细胞/基因扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01127v1",
    "title": "Wittgenstein's Family Resemblance Clustering Algorithm",
    "summary": "This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.",
    "authors": [
      "Golbahar Amanpour",
      "Benyamin Ghojogh"
    ],
    "url": "http://arxiv.org/abs/2601.01127v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于维特根斯坦家族相似性哲学概念的新型聚类算法，属于机器学习方法论的创新，而非将AI应用于具体科学领域发现或扰动预测研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01123v1",
    "title": "Learning from Historical Activations in Graph Neural Networks",
    "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.",
    "authors": [
      "Yaniv Galron",
      "Hadar Sinai",
      "Haggai Maron",
      "Moshe Eliasof"
    ],
    "url": "http://arxiv.org/abs/2601.01123v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种利用历史激活的图神经网络聚合方法HISTOGRAPH，可应用于分子化学等科学发现领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01119v1",
    "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings",
    "summary": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.",
    "authors": [
      "Muhammad Ashad Kabir",
      "Sirajam Munira",
      "Dewan Tasnia Azad",
      "Saleh Mohammed Ikram",
      "Mohammad Habibur Rahman Sarker",
      "Syed Manzoor Ahmed Hanifi"
    ],
    "url": "http://arxiv.org/abs/2601.01119v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文开发了一种可解释的机器学习框架，用于在低资源环境中进行社区早期慢性肾病筛查，属于AI在生物医学领域的科学应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01118v1",
    "title": "ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services",
    "summary": "The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.",
    "authors": [
      "Qingqing Long",
      "Haotian Chen",
      "Chenyang Zhao",
      "Xiaolei Du",
      "Xuezhi Wang",
      "Pengyao Wang",
      "Chengzan Li",
      "Yuanchun Zhou",
      "Hengshu Zhu"
    ],
    "url": "http://arxiv.org/abs/2601.01118v1",
    "published": "2026-01-03",
    "primary_category": "cs.IR",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于大语言模型的对话式推荐系统，专门用于促进大规模科学数据共享服务，而非直接进行生物学或化学等领域的科学发现或扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01116v1",
    "title": "Beyond P-Values: Importing Quantitative Finance's Risk and Regret Metrics for AI in Learning Health Systems",
    "summary": "The increasing deployment of artificial intelligence (AI) in clinical settings challenges foundational assumptions underlying traditional frameworks of medical evidence. Classical statistical approaches, centered on randomized controlled trials, frequentist hypothesis testing, and static confidence intervals, were designed for fixed interventions evaluated under stable conditions. In contrast, AI-driven clinical systems learn continuously, adapt their behavior over time, and operate in non-stationary environments shaped by evolving populations, practices, and feedback effects. In such systems, clinical harm arises less from average error rates than from calibration drift, rare but severe failures, and the accumulation of suboptimal decisions over time.   In this perspective, we argue that prevailing notions of statistical significance are insufficient for characterizing evidence and safety in learning health systems. Drawing on risk-theoretic concepts from quantitative finance and online decision theory, we propose reframing medical evidence for adaptive AI systems in terms of time-indexed calibration stability, bounded downside risk, and controlled cumulative regret. We emphasize that this approach does not replace randomized trials or causal inference, but complements them by addressing dimensions of risk and uncertainty that emerge only after deployment. This framework provides a principled mathematical language for evaluating AI-driven clinical systems under continual learning and offers implications for clinical practice, research design, and regulatory oversight.",
    "authors": [
      "Richik Chakraborty"
    ],
    "url": "http://arxiv.org/abs/2601.01116v1",
    "published": "2026-01-03",
    "primary_category": "stat.ME",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出将金融风险度量方法应用于临床AI系统的持续学习评估框架，而非利用AI进行基础科学发现或分子扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01100v1",
    "title": "Strain-triggered high-temperature superconducting transition in two-dimensional carbon allotrope",
    "summary": "Driving non-superconducting materials into a superconducting state through specific modulation is a key focus in the field of superconductivity. Pressure is a powerful method that can switch a three-dimensional (3D) material between non-superconducting and superconducting states. In the two-dimensional (2D) case, strain engineering plays a similar role to pressure. However, purely strain-induced superconductivity in 2D systems remains exceedingly scarce. Using first-principles calculations, we demonstrate that a superconducting transition can be induced solely by applying biaxial tensile strain in a 2D carbon allotrope, THO-graphene, which is composed of triangles, hexagons, and octagons. Free-standing THO-graphene is non-superconducting. Surprisingly, the electron-phonon coupling in strained THO-graphene is enhanced strong enough to pair electrons and realize superconductivity, with the highest superconducting transition temperature reaching 45 K. This work not only provides a notable example of controlling metal-superconductor transition in 2D system just via strain, but also sets a new record of superconducting transition temperature for 2D elemental superconductors.",
    "authors": [
      "Tian Yan",
      "Ru Zheng",
      "Jin-Hua Sun",
      "Fengjie Ma",
      "Xun-Wang Yan",
      "Miao Gao",
      "Tian Cui",
      "Zhong-Yi Lu"
    ],
    "url": "http://arxiv.org/abs/2601.01100v1",
    "published": "2026-01-03",
    "primary_category": "cond-mat.supr-con",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文通过第一性原理计算研究二维碳同素异形体的应变诱导超导转变，属于传统计算物理研究，未涉及人工智能方法或细胞/分子层面的扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01099v1",
    "title": "Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks",
    "summary": "This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.",
    "authors": [
      "Mahmudul Hasan",
      "Mabsur Fatin Bin Hossain"
    ],
    "url": "http://arxiv.org/abs/2601.01099v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于计算机视觉领域的卷积神经网络架构比较与优化，未涉及生物学、化学或物理学等自然科学领域的科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01097v1",
    "title": "Neural Networks on Symmetric Spaces of Noncompact Type",
    "summary": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.",
    "authors": [
      "Xuan Son Nguyen",
      "Shuo Yang",
      "Aymeric Histace"
    ],
    "url": "http://arxiv.org/abs/2601.01097v1",
    "published": "2026-01-03",
    "primary_category": "stat.ML",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种在非紧型对称空间上构建神经网络的新方法，通过统一距离公式设计了全连接层和注意力机制，属于机器学习基础理论研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01095v1",
    "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame",
    "summary": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.",
    "authors": [
      "Hyeonjeong Ha",
      "Jinjin Ge",
      "Bo Feng",
      "Kaixin Ma",
      "Gargi Chakraborty"
    ],
    "url": "http://arxiv.org/abs/2601.01095v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了首个评估视频语言模型叙事理解能力的基准框架，专注于多模态人工智能的实体追踪与时间推理能力，而非科学发现或生物扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01094v1",
    "title": "SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking",
    "summary": "Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.",
    "authors": [
      "Yubo Shu",
      "Peng Zhang",
      "Meng Wu",
      "Yan Chen",
      "Haoxuan Zhou",
      "Guanming Liu",
      "Yu Zhang",
      "Liuxin Zhang",
      "Qianying Wang",
      "Tun Lu",
      "Ning Gu"
    ],
    "url": "http://arxiv.org/abs/2601.01094v1",
    "published": "2026-01-03",
    "primary_category": "cs.HC",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究在LLM搜索系统中整合社交线索对用户信息行为的影响，属于人机交互与信息检索领域，不涉及科学发现或细胞扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01091v1",
    "title": "ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining",
    "summary": "Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.",
    "authors": [
      "Haq Nawaz Malik"
    ],
    "url": "http://arxiv.org/abs/2601.01091v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于为克什米尔语构建大规模文本数据集以训练语言模型，属于自然语言处理资源建设，而非将AI应用于自然科学发现或细胞/分子扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01090v1",
    "title": "Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai",
    "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.   We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.   These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.",
    "authors": [
      "Erica Coppolillo",
      "Luca Luceri",
      "Emilio Ferrara"
    ],
    "url": "http://arxiv.org/abs/2601.01090v1",
    "published": "2026-01-03",
    "primary_category": "cs.MA",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文研究AI社交平台中语言模型代理的毒性行为传播机制，属于AI社会行为分析而非传统科学发现领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01089v1",
    "title": "Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding",
    "summary": "Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.",
    "authors": [
      "Nobuyuki Ota"
    ],
    "url": "http://arxiv.org/abs/2601.01089v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": true,
    "reasoning": "该论文通过开发一种整合DNA、RNA和蛋白质信息的AI架构来预测CRISPRi增强子扰动对细胞的影响，实现了生物学信息流的机制导向建模。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01085v1",
    "title": "Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models",
    "summary": "In this paper, we introduce \\emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \\emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.",
    "authors": [
      "Jiayi Xu",
      "Zhang Zhang",
      "Yuanrui Zhang",
      "Ruitao Chen",
      "Yixian Xu",
      "Tianyu He",
      "Di He"
    ],
    "url": "http://arxiv.org/abs/2601.01085v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于视觉生成模型的免训练、概率认证水印方法，不涉及科学发现或细胞/基因扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01082v1",
    "title": "Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces",
    "summary": "Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.",
    "authors": [
      "Bryon Tjanaka",
      "Henry Chen",
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "url": "http://arxiv.org/abs/2601.01082v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于高维度量空间质量多样性优化的折扣模型搜索方法，主要贡献在于改进优化算法而非特定科学领域的发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01076v1",
    "title": "Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees",
    "summary": "We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.",
    "authors": [
      "Devesh Nath",
      "Haoran Yin",
      "Glen Chou"
    ],
    "url": "http://arxiv.org/abs/2601.01076v1",
    "published": "2026-01-03",
    "primary_category": "eess.SY",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于Koopman算子的数据驱动可达性分析框架，通过神经网络提升函数和保形预测保证，用于未知非线性动力系统的安全验证与控制，属于控制理论与机器学习交叉领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01075v1",
    "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "summary": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.",
    "authors": [
      "Hansen Jin Lillemark",
      "Benhao Huang",
      "Fangneng Zhan",
      "Yilun Du",
      "Thomas Anderson Keller"
    ],
    "url": "http://arxiv.org/abs/2601.01075v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种利用流等变对称性改进部分观测动态环境中世界建模的神经网络框架，属于计算机视觉与机器人领域的算法研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01073v1",
    "title": "Gendered Pathways in AI Companionship: Cross-Community Behavior and Toxicity Patterns on Reddit",
    "summary": "AI-companionship platforms are rapidly reshaping how people form emotional, romantic, and parasocial bonds with non-human agents, raising new questions about how these relationships intersect with gendered online behavior and exposure to harmful content. Focusing on the MyBoyfriendIsAI (MBIA) subreddit, we reconstruct the Reddit activity histories of more than 3,000 highly engaged users over two years, yielding over 67,000 historical submissions. We then situate MBIA within a broader ecosystem by building a historical interaction network spanning more than 2,000 subreddits, which enables us to trace cross-community pathways and measure how toxicity and emotional expression vary across these trajectories. We find that MBIA users primarily traverse four surrounding community spheres (AI-companionship, porn-related, forum-like, and gaming) and that participation across the ecosystem exhibits a distinct gendered structure, with substantial engagement by female users. While toxicity is generally low across most pathways, we observe localized spikes concentrated in a small subset of AI-porn and gender-oriented communities. Nearly 16% of users engage with gender-focused subreddits, and their trajectories display systematically different patterns of emotional expression and elevated toxicity, suggesting that a minority of gendered pathways may act as toxicity amplifiers within the broader AI-companionship ecosystem. These results characterize the gendered structure of cross-community participation around AI companionship on Reddit and highlight where risks concentrate, informing measurement, moderation, and design practices for human-AI relationship platforms.",
    "authors": [
      "Erica Coppolillo",
      "Emilio Ferrara"
    ],
    "url": "http://arxiv.org/abs/2601.01073v1",
    "published": "2026-01-03",
    "primary_category": "cs.SI",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该研究分析Reddit平台上AI伴侣用户的跨社区行为模式与毒性分布，属于计算社会科学范畴而非传统自然科学领域的AI4Science或扰动预测研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01069v1",
    "title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs",
    "summary": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \\emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.",
    "authors": [
      "Jing Wang",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ],
    "url": "http://arxiv.org/abs/2601.01069v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于改进非平稳参数化赌博机和马尔可夫决策过程的加权策略算法框架与理论分析，属于机器学习理论领域，未涉及具体科学发现或扰动预测应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01065v1",
    "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco",
    "summary": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.",
    "authors": [
      "Achraf Hsain",
      "Yahya Zaki",
      "Othman Abaakil",
      "Hibat-allah Bekkar",
      "Yousra Chtouki"
    ],
    "url": "http://arxiv.org/abs/2601.01065v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文将微型机器学习应用于水产养殖监测，通过实时数据采集和异常检测优化养殖环境管理，属于AI在生物学应用领域的科学实践。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01062v1",
    "title": "SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\\% win rate) and narrative depth (+50\\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).",
    "authors": [
      "Yunlin Zeng"
    ],
    "url": "http://arxiv.org/abs/2601.01062v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于视觉语言模型在生成多说话者播客对话方面的能力评估，属于人工智能在自然语言生成和视觉理解领域的应用研究，而非科学发现或细胞扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01061v1",
    "title": "A UCB Bandit Algorithm for General ML-Based Estimators",
    "summary": "We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB",
    "authors": [
      "Yajing Liu",
      "Erkao Bao",
      "Linqi Song"
    ],
    "url": "http://arxiv.org/abs/2601.01061v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种通用的机器学习置信上界算法，用于将任意机器学习模型集成到多臂老虎机框架中，而非针对特定科学领域或扰动预测应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01056v1",
    "title": "Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance",
    "summary": "The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\\% and an average AUC of 96.8\\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\\% and accuracy of 99.84\\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.",
    "authors": [
      "Ifeanyi Ezuma",
      "Ugochukwu Ugwu"
    ],
    "url": "http://arxiv.org/abs/2601.01056v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文使用AI方法（机器学习和深度学习）对组织病理学图像进行分类，属于AI在生物医学科学发现中的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01055v1",
    "title": "Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics",
    "summary": "This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.",
    "authors": [
      "Ernest Fokoué"
    ],
    "url": "http://arxiv.org/abs/2601.01055v1",
    "published": "2026-01-03",
    "primary_category": "stat.ML",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文专注于机器学习算法理论，提出基于斐波那契递归的集成学习框架，不涉及自然科学领域的科学发现或细胞/基因扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01053v1",
    "title": "Byzantine-Robust Federated Learning Framework with Post-Quantum Secure Aggregation for Real-Time Threat Intelligence Sharing in Critical IoT Infrastructure",
    "summary": "The proliferation of Internet of Things devices in critical infrastructure has created unprecedented cybersecurity challenges, necessitating collaborative threat detection mechanisms that preserve data privacy while maintaining robustness against sophisticated attacks. Traditional federated learning approaches for IoT security suffer from two critical vulnerabilities: susceptibility to Byzantine attacks where malicious participants poison model updates, and inadequacy against future quantum computing threats that can compromise cryptographic aggregation protocols. This paper presents a novel Byzantine-robust federated learning framework integrated with post-quantum secure aggregation specifically designed for real-time threat intelligence sharing across critical IoT infrastructure. The proposed framework combines a adaptive weighted aggregation mechanism with lattice-based cryptographic protocols to simultaneously defend against model poisoning attacks and quantum adversaries. We introduce a reputation-based client selection algorithm that dynamically identifies and excludes Byzantine participants while maintaining differential privacy guarantees. The secure aggregation protocol employs CRYSTALS-Kyber for key encapsulation and homomorphic encryption to ensure confidentiality during parameter updates. Experimental evaluation on industrial IoT intrusion detection datasets demonstrates that our framework achieves 96.8% threat detection accuracy while successfully mitigating up to 40% Byzantine attackers, with only 18% computational overhead compared to non-secure federated approaches. The framework maintains sub-second aggregation latency suitable for real-time applications and provides 256-bit post-quantum security level.",
    "authors": [
      "Milad Rahmati",
      "Nima Rahmati"
    ],
    "url": "http://arxiv.org/abs/2601.01053v1",
    "published": "2026-01-03",
    "primary_category": "cs.CR",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种针对关键物联网基础设施的拜占庭鲁棒联邦学习框架，结合后量子安全聚合技术，用于实时威胁情报共享，而非应用于生物学、化学或物理学等科学发现领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01050v1",
    "title": "EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos",
    "summary": "We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.",
    "authors": [
      "Hongming Fu",
      "Wenjia Wang",
      "Xiaozhen Qiao",
      "Shuo Yang",
      "Zheng Liu",
      "Bo Zhao"
    ],
    "url": "http://arxiv.org/abs/2601.01050v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种从第一人称视频中重建世界空间手-物体交互的计算机视觉方法，属于计算机视觉和人工智能应用领域，而非生物学、化学或物理学等传统科学发现。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01045v1",
    "title": "Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI",
    "summary": "Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.   In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.",
    "authors": [
      "Tatsuaki Tsuruyama"
    ],
    "url": "http://arxiv.org/abs/2601.01045v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于信息论Lyapunov函数的扩散模型控制方法，用于在图像生成过程中保持粗粒度统计特性，属于生成式AI的理论改进而非科学发现应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01044v1",
    "title": "Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data",
    "summary": "Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.",
    "authors": [
      "Jin Wang",
      "Angelo De Castro",
      "Yuxi Zhang",
      "Lucas Basolli Borsatto",
      "Yuechen Guo",
      "Victoria Bastos Primo",
      "Ana Beatriz Montevecchio Bernardino",
      "Gota Morota",
      "Ricardo C Chebel",
      "Haipeng Yu"
    ],
    "url": "http://arxiv.org/abs/2601.01044v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该研究使用深度学习模型（ConvNeXt、MobileViT、PointNet、DGCNN）和迁移学习策略，通过深度图像和点云数据预测奶牛体重，属于农业科学领域的AI应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01037v1",
    "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation",
    "summary": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.",
    "authors": [
      "Livia Leong Hui Teng"
    ],
    "url": "http://arxiv.org/abs/2601.01037v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种多维度提示链框架，用于提升小型语言模型在开放域对话生成中的人类相似性，属于自然语言处理领域而非AI4Science或扰动预测范畴。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01029v1",
    "title": "Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights",
    "summary": "This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.",
    "authors": [
      "Zeyu Bian",
      "Max Biggs",
      "Ruijiang Gao",
      "Zhengling Qi"
    ],
    "url": "http://arxiv.org/abs/2601.01029v1",
    "published": "2026-01-03",
    "primary_category": "stat.ML",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种使用累积倾向权重评估AI驱动决策（如定价和贷款）中消费者剩余的新方法，属于经济学和机器学习交叉领域，而非AI4Science或扰动预测范畴。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01027v1",
    "title": "A Platform for Interactive AI Character Experiences",
    "summary": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
    "authors": [
      "Rafael Wampfler",
      "Chen Yang",
      "Dillon Elste",
      "Nikola Kovacevic",
      "Philine Witzig",
      "Markus Gross"
    ],
    "url": "http://arxiv.org/abs/2601.01027v1",
    "published": "2026-01-03",
    "primary_category": "cs.HC",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出一个集成多种AI技术的交互式数字角色平台，专注于对话系统和角色体验构建，而非科学发现或细胞扰动预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01026v1",
    "title": "Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation",
    "summary": "We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.",
    "authors": [
      "Douglas Costa Braga",
      "Daniel Oliveira Dantas"
    ],
    "url": "http://arxiv.org/abs/2601.01026v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文使用注意力机制CNN进行白血病细胞分类，属于AI在生物医学图像分析领域的应用，但不涉及细胞扰动响应的预测。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01024v1",
    "title": "ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval",
    "summary": "Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself",
    "authors": [
      "Tien-Huy Nguyen",
      "Huu-Loc Tran",
      "Thanh Duc Ngo"
    ],
    "url": "http://arxiv.org/abs/2601.01024v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种基于注意力引导的细粒度对齐框架，用于提升视觉语言检索中文本-图像匹配的精确度，属于计算机视觉与自然语言处理的交叉领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01023v1",
    "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning",
    "summary": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.",
    "authors": [
      "João Morais",
      "Sadjad Alikhani",
      "Akshay Malhotra",
      "Shahab Hamidi-Rad",
      "Ahmed Alkhateeb"
    ],
    "url": "http://arxiv.org/abs/2601.01023v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于评估无线通信数据集相似性的任务感知框架，通过度量数据集距离来预测模型跨数据集的可迁移性，属于机器学习在通信工程领域的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01022v1",
    "title": "Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking",
    "summary": "Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking",
    "authors": [
      "Shiao Wang",
      "Xiao Wang",
      "Haonan Zhao",
      "Jiarui Xu",
      "Bo Jiang",
      "Lin Zhu",
      "Xin Zhao",
      "Yonghong Tian",
      "Jin Tang"
    ],
    "url": "http://arxiv.org/abs/2601.01022v1",
    "published": "2026-01-03",
    "primary_category": "cs.CV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种在频域解耦振幅和相位注意力的RGB-事件视觉目标跟踪框架，属于计算机视觉领域的算法改进研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01021v1",
    "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations",
    "summary": "Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.",
    "authors": [
      "Dai Shi",
      "Lequan Lin",
      "Andi Han",
      "Luke Thompson",
      "José Miguel Hernández-Lobato",
      "Zhiyong Wang",
      "Junbin Gao"
    ],
    "url": "http://arxiv.org/abs/2601.01021v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文提出基于Wiener混沌展开的神经算子架构，用于学习随机微分方程和随机偏微分方程的解算子，在洪水预测等多个科学领域展示了应用潜力。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01016v1",
    "title": "Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study",
    "summary": "In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.",
    "authors": [
      "Ata Akbari Asanjan",
      "Milad Memarzadeh",
      "Bryan Matthews",
      "Nikunj Oza"
    ],
    "url": "http://arxiv.org/abs/2601.01016v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文主要研究使用随机傅里叶变换改进变分自编码器在航空安全异常检测中的应用，属于机器学习方法优化领域，而非特定科学发现或扰动预测研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01014v1",
    "title": "Geometric and Dynamic Scaling in Deep Transformers",
    "summary": "Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.",
    "authors": [
      "Haoran Su",
      "Chenyu You"
    ],
    "url": "http://arxiv.org/abs/2601.01014v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种几何框架来解决深度Transformer中的表示崩溃问题，属于深度学习架构改进的基础研究。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01011v1",
    "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models",
    "summary": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies",
    "authors": [
      "Patricio Vera"
    ],
    "url": "http://arxiv.org/abs/2601.01011v1",
    "published": "2026-01-03",
    "primary_category": "cs.CL",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出意图塌缩理论框架和三个模型无关的意图度量指标，用于研究语言模型推理过程中内部意图在语言化前的形成机制。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01010v1",
    "title": "Disordered Dynamics in High Dimensions: Connections to Random Matrices and Machine Learning",
    "summary": "We provide an overview of high dimensional dynamical systems driven by random matrices, focusing on applications to simple models of learning and generalization in machine learning theory. Using both cavity method arguments and path integrals, we review how the behavior of a coupled infinite dimensional system can be characterized as a stochastic process for each single site of the system. We provide a pedagogical treatment of dynamical mean field theory (DMFT), a framework that can be flexibly applied to these settings. The DMFT single site stochastic process is fully characterized by a set of (two-time) correlation and response functions. For linear time-invariant systems, we illustrate connections between random matrix resolvents and the DMFT response. We demonstrate applications of these ideas to machine learning models such as gradient flow, stochastic gradient descent on random feature models and deep linear networks in the feature learning regime trained on random data. We demonstrate how bias and variance decompositions (analysis of ensembling/bagging etc) can be computed by averaging over subsets of the DMFT noise variables. From our formalism we also investigate how linear systems driven with random non-Hermitian matrices (such as random feature models) can exhibit non-monotonic loss curves with training time, while Hermitian matrices with the matching spectra do not, highlighting a different mechanism for non-monotonicity than small eigenvalues causing instability to label noise. Lastly, we provide asymptotic descriptions of the training and test loss dynamics for randomly initialized deep linear neural networks trained in the feature learning regime with high-dimensional random data. In this case, the time translation invariance structure is lost and the hidden layer weights are characterized as spiked random matrices.",
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "url": "http://arxiv.org/abs/2601.01010v1",
    "published": "2026-01-03",
    "primary_category": "cond-mat.dis-nn",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文主要研究高维动力系统的随机矩阵理论及其在机器学习模型训练动态分析中的应用，属于理论机器学习范畴，而非AI4Science或扰动预测领域。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01009v1",
    "title": "Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms",
    "summary": "This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.",
    "authors": [
      "Mojtaba Aliasghar-Mamaghani",
      "Mohammadreza Khalafi"
    ],
    "url": "http://arxiv.org/abs/2601.01009v1",
    "published": "2026-01-03",
    "primary_category": "cs.LG",
    "is_ai4science": true,
    "is_perturbation": false,
    "reasoning": "该论文使用多种机器学习算法分析混凝土成分对氯离子传输的影响，属于AI在材料科学领域的应用，旨在通过数据驱动方法揭示物理过程并预测基础设施寿命。"
  },
  {
    "id": "http://arxiv.org/abs/2601.01008v1",
    "title": "An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions",
    "summary": "Artificial intelligence models have shown strong potential in acute ischemic stroke imaging, particularly for lesion detection and segmentation using computed tomography and magnetic resonance imaging. However, most existing approaches operate as black box predictors, producing deterministic outputs without explicit uncertainty awareness or structured mechanisms to abstain under ambiguous conditions. This limitation raises serious safety and trust concerns in high risk emergency radiology settings. In this paper, we propose an explainable agentic AI framework for uncertainty aware and abstention enabled decision support in acute ischemic stroke imaging. The framework follows a modular agentic pipeline in which a perception agent performs lesion aware image analysis, an uncertainty estimation agent computes slice level predictive reliability, and a decision agent determines whether to issue a prediction or abstain based on predefined uncertainty thresholds. Unlike prior stroke imaging systems that primarily focus on improving segmentation or classification accuracy, the proposed framework explicitly prioritizes clinical safety, transparency, and clinician aligned decision behavior. Qualitative and case based analyses across representative stroke imaging scenarios demonstrate that uncertainty driven abstention naturally emerges in diagnostically ambiguous regions and low information slices. The framework further integrates visual explanation mechanisms to support both predictive and abstention decisions, addressing a key limitation of existing uncertainty aware medical imaging systems. Rather than introducing a new performance benchmark, this work presents agentic control, uncertainty awareness, and selective abstention as essential design principles for developing safe and trustworthy medical imaging AI systems.",
    "authors": [
      "Md Rashadul Islam"
    ],
    "url": "http://arxiv.org/abs/2601.01008v1",
    "published": "2026-01-03",
    "primary_category": "eess.IV",
    "is_ai4science": false,
    "is_perturbation": false,
    "reasoning": "该论文提出了一种用于急性缺血性卒中影像决策的可解释AI框架，重点在于临床安全性和不确定性管理，而非科学发现或分子扰动预测。"
  }
]